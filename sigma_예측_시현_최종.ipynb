{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "dpCvBQkVlC2r",
        "outputId": "c20bf30e-fdb9-4936-a20f-f28f41106ccf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "최종 shape: (104234, 90)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "     hhid  com  weight  strata  utl_exp_ppp17    male  hsize  num_children5  \\\n",
              "0  100001    1      75       4      594.80627  Female      1              0   \n",
              "1  100002    1     150       4     1676.27230  Female      2              0   \n",
              "2  100003    1     375       4      506.93719    Male      5              0   \n",
              "3  100004    1     375       4      824.61786    Male      5              0   \n",
              "4  100005    1     525       4      351.47644    Male      7              1   \n",
              "\n",
              "   num_children10  num_children18  ...  consumed4400 consumed4500  \\\n",
              "0               0               0  ...            No           No   \n",
              "1               0               0  ...            No           No   \n",
              "2               0               2  ...            No          Yes   \n",
              "3               0               1  ...            No           No   \n",
              "4               0               0  ...            No          Yes   \n",
              "\n",
              "  consumed4600 consumed4700 consumed4800 consumed4900 consumed5000  \\\n",
              "0          Yes          Yes          Yes          Yes           No   \n",
              "1           No          Yes          Yes           No           No   \n",
              "2          Yes          Yes          Yes           No          Yes   \n",
              "3           No          Yes          Yes           No           No   \n",
              "4           No          Yes          Yes          Yes           No   \n",
              "\n",
              "  survey_id_x cons_ppp17  survey_id_y  \n",
              "0      100000  25.258402       100000  \n",
              "1      100000  16.996706       100000  \n",
              "2      100000  13.671848       100000  \n",
              "3      100000   7.189475       100000  \n",
              "4      100000  12.308855       100000  \n",
              "\n",
              "[5 rows x 90 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-85133e99-77a2-40c8-aace-bfea4857a0b7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hhid</th>\n",
              "      <th>com</th>\n",
              "      <th>weight</th>\n",
              "      <th>strata</th>\n",
              "      <th>utl_exp_ppp17</th>\n",
              "      <th>male</th>\n",
              "      <th>hsize</th>\n",
              "      <th>num_children5</th>\n",
              "      <th>num_children10</th>\n",
              "      <th>num_children18</th>\n",
              "      <th>...</th>\n",
              "      <th>consumed4400</th>\n",
              "      <th>consumed4500</th>\n",
              "      <th>consumed4600</th>\n",
              "      <th>consumed4700</th>\n",
              "      <th>consumed4800</th>\n",
              "      <th>consumed4900</th>\n",
              "      <th>consumed5000</th>\n",
              "      <th>survey_id_x</th>\n",
              "      <th>cons_ppp17</th>\n",
              "      <th>survey_id_y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>100001</td>\n",
              "      <td>1</td>\n",
              "      <td>75</td>\n",
              "      <td>4</td>\n",
              "      <td>594.80627</td>\n",
              "      <td>Female</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>100000</td>\n",
              "      <td>25.258402</td>\n",
              "      <td>100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>100002</td>\n",
              "      <td>1</td>\n",
              "      <td>150</td>\n",
              "      <td>4</td>\n",
              "      <td>1676.27230</td>\n",
              "      <td>Female</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>100000</td>\n",
              "      <td>16.996706</td>\n",
              "      <td>100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100003</td>\n",
              "      <td>1</td>\n",
              "      <td>375</td>\n",
              "      <td>4</td>\n",
              "      <td>506.93719</td>\n",
              "      <td>Male</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>100000</td>\n",
              "      <td>13.671848</td>\n",
              "      <td>100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>100004</td>\n",
              "      <td>1</td>\n",
              "      <td>375</td>\n",
              "      <td>4</td>\n",
              "      <td>824.61786</td>\n",
              "      <td>Male</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>100000</td>\n",
              "      <td>7.189475</td>\n",
              "      <td>100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>100005</td>\n",
              "      <td>1</td>\n",
              "      <td>525</td>\n",
              "      <td>4</td>\n",
              "      <td>351.47644</td>\n",
              "      <td>Male</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>100000</td>\n",
              "      <td>12.308855</td>\n",
              "      <td>100000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 90 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-85133e99-77a2-40c8-aace-bfea4857a0b7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-85133e99-77a2-40c8-aace-bfea4857a0b7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-85133e99-77a2-40c8-aace-bfea4857a0b7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "정리 후 컬럼명: True\n",
            "최종 shape: (104234, 89)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     hhid  com  weight  strata  utl_exp_ppp17    male  hsize  num_children5  \\\n",
              "0  100001    1      75       4      594.80627  Female      1              0   \n",
              "1  100002    1     150       4     1676.27230  Female      2              0   \n",
              "2  100003    1     375       4      506.93719    Male      5              0   \n",
              "3  100004    1     375       4      824.61786    Male      5              0   \n",
              "4  100005    1     525       4      351.47644    Male      7              1   \n",
              "\n",
              "   num_children10  num_children18  ...  consumed4300 consumed4400  \\\n",
              "0               0               0  ...            No           No   \n",
              "1               0               0  ...            No           No   \n",
              "2               0               2  ...           Yes           No   \n",
              "3               0               1  ...           Yes           No   \n",
              "4               0               0  ...            No           No   \n",
              "\n",
              "  consumed4500 consumed4600 consumed4700 consumed4800 consumed4900  \\\n",
              "0           No          Yes          Yes          Yes          Yes   \n",
              "1           No           No          Yes          Yes           No   \n",
              "2          Yes          Yes          Yes          Yes           No   \n",
              "3           No           No          Yes          Yes           No   \n",
              "4          Yes           No          Yes          Yes          Yes   \n",
              "\n",
              "  consumed5000 survey_id  cons_ppp17  \n",
              "0           No    100000   25.258402  \n",
              "1           No    100000   16.996706  \n",
              "2          Yes    100000   13.671848  \n",
              "3           No    100000    7.189475  \n",
              "4           No    100000   12.308855  \n",
              "\n",
              "[5 rows x 89 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-82b5dcdb-9adc-4a2c-a404-854345e61833\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hhid</th>\n",
              "      <th>com</th>\n",
              "      <th>weight</th>\n",
              "      <th>strata</th>\n",
              "      <th>utl_exp_ppp17</th>\n",
              "      <th>male</th>\n",
              "      <th>hsize</th>\n",
              "      <th>num_children5</th>\n",
              "      <th>num_children10</th>\n",
              "      <th>num_children18</th>\n",
              "      <th>...</th>\n",
              "      <th>consumed4300</th>\n",
              "      <th>consumed4400</th>\n",
              "      <th>consumed4500</th>\n",
              "      <th>consumed4600</th>\n",
              "      <th>consumed4700</th>\n",
              "      <th>consumed4800</th>\n",
              "      <th>consumed4900</th>\n",
              "      <th>consumed5000</th>\n",
              "      <th>survey_id</th>\n",
              "      <th>cons_ppp17</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>100001</td>\n",
              "      <td>1</td>\n",
              "      <td>75</td>\n",
              "      <td>4</td>\n",
              "      <td>594.80627</td>\n",
              "      <td>Female</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>100000</td>\n",
              "      <td>25.258402</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>100002</td>\n",
              "      <td>1</td>\n",
              "      <td>150</td>\n",
              "      <td>4</td>\n",
              "      <td>1676.27230</td>\n",
              "      <td>Female</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>100000</td>\n",
              "      <td>16.996706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100003</td>\n",
              "      <td>1</td>\n",
              "      <td>375</td>\n",
              "      <td>4</td>\n",
              "      <td>506.93719</td>\n",
              "      <td>Male</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>100000</td>\n",
              "      <td>13.671848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>100004</td>\n",
              "      <td>1</td>\n",
              "      <td>375</td>\n",
              "      <td>4</td>\n",
              "      <td>824.61786</td>\n",
              "      <td>Male</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>100000</td>\n",
              "      <td>7.189475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>100005</td>\n",
              "      <td>1</td>\n",
              "      <td>525</td>\n",
              "      <td>4</td>\n",
              "      <td>351.47644</td>\n",
              "      <td>Male</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>100000</td>\n",
              "      <td>12.308855</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 89 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-82b5dcdb-9adc-4a2c-a404-854345e61833')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-82b5dcdb-9adc-4a2c-a404-854345e61833 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-82b5dcdb-9adc-4a2c-a404-854345e61833');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "train_hh_features = pd.read_csv('/content/drive/MyDrive/Poverty_prediction_challenge/train_hh_features.csv')\n",
        "train_hh_gt = pd.read_csv('/content/drive/MyDrive/Poverty_prediction_challenge/train_hh_gt.csv')\n",
        "train_rates_gt = pd.read_csv('/content/drive/MyDrive/Poverty_prediction_challenge/train_rates_gt.csv')\n",
        "test_hh_features = pd.read_csv('/content/drive/MyDrive/Poverty_prediction_challenge/test_hh_features.csv')\n",
        "\n",
        "df = train_hh_features.merge(\n",
        "    train_hh_gt[['hhid', 'cons_ppp17', 'survey_id']],\n",
        "    on='hhid',\n",
        "    how='inner',\n",
        "    validate='one_to_one'\n",
        ")\n",
        "\n",
        "if 'survey_id' in df.columns:\n",
        "    print(\"survey_id 컬럼이 성공적으로 병합되었습니다.\")\n",
        "\n",
        "print(\"최종 shape:\", df.shape)\n",
        "display(df.head())\n",
        "\n",
        "assert (df['survey_id_x'] == df['survey_id_y']).all()\n",
        "\n",
        "df = df.drop(columns=['survey_id_y'])\n",
        "df = df.rename(columns={'survey_id_x': 'survey_id'})\n",
        "\n",
        "print(\"정리 후 컬럼명:\", 'survey_id' in df.columns)\n",
        "print(\"최종 shape:\", df.shape)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SqnqzMtms6e",
        "outputId": "d3d89fde-6072-4242-d690-e8222959fda2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from catboost) (1.16.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (3.3.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (9.1.2)\n",
            "Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "def prepare_features(df: pd.DataFrame, state: dict = None, fit: bool = False):\n",
        "\n",
        "    df = df.copy()\n",
        "    state = {} if state is None else state\n",
        "\n",
        "     # 1) hsize 불일치 수정(옵션 D1)\n",
        "    comp_sum = (df[\"num_children5\"] + df[\"num_children10\"] + df[\"num_children18\"] +\n",
        "                df[\"num_adult_female\"] + df[\"num_adult_male\"] + df[\"num_elderly\"])\n",
        "    inconsistent = df[\"hsize\"] < comp_sum\n",
        "    df[\"hsize_fixed\"] = df[\"hsize\"].where(~inconsistent, comp_sum)\n",
        "\n",
        "    #agri\n",
        "    # Check if 'sector1d' column exists before processing\n",
        "    if \"sector1d\" in df.columns:\n",
        "        s = df[\"sector1d\"]\n",
        "        # 결측치 플래그\n",
        "        df[\"is_sector_missing\"] = s.isna().astype(int)\n",
        "        # 문자열 정리(공백, 대소문자, NaN 안전)\n",
        "        s_clean = s.fillna(\"\").astype(str).str.strip()\n",
        "        # Agriculture, hunting and forestry / Fishing\n",
        "        df[\"is_agri\"] = (s_clean == \"Agriculture, hunting and forestry\").astype(int)\n",
        "        df[\"is_fish\"] = (s_clean == \"Fishing\").astype(int)\n",
        "        # 합친 플래그\n",
        "        df[\"is_agri_fish\"] = ((df[\"is_agri\"] == 1) | (df[\"is_fish\"] == 1)).astype(int)\n",
        "    else:\n",
        "        # If 'sector1d' is not present, create the columns with default values\n",
        "        df[\"is_sector_missing\"] = 1 # Assume missing if column is absent\n",
        "        df[\"is_agri\"] = 0\n",
        "        df[\"is_fish\"] = 0\n",
        "        df[\"is_agri_fish\"] = 0\n",
        "\n",
        "    #binary\n",
        "    binary_cols = [\n",
        "        \"male\", \"owner\", \"water\", \"toilet\", \"sewer\", \"elect\",\n",
        "        \"employed\", \"urban\", \"any_nonagric\"\n",
        "    ]\n",
        "    consumed_cols = [c for c in df.columns if c.startswith(\"consumed\")]\n",
        "    binary_cols_extended = [c for c in binary_cols if c in df.columns] + consumed_cols\n",
        "\n",
        "    def to_binary_series(s):\n",
        "        if pd.api.types.is_numeric_dtype(s):\n",
        "            return s.fillna(0).astype(int)\n",
        "\n",
        "        mapping = {\n",
        "            \"Yes\": 1, \"No\": 0,\n",
        "            \"Male\": 1, \"Female\": 0,\n",
        "            \"Access\": 1, \"No access\": 0,\n",
        "            \"Employed\": 1, \"Not employed\": 0,\n",
        "            \"Urban\": 1, \"Rural\": 0,\n",
        "            \"Owner\": 1, \"Not owner\": 0\n",
        "        }\n",
        "        return s.map(mapping).fillna(0).astype(int)\n",
        "\n",
        "    for col in binary_cols_extended:\n",
        "        df[col] = to_binary_series(df[col])\n",
        "\n",
        "    #label encoding\n",
        "    if \"educ_max\" in df.columns:\n",
        "        educ_map = {\n",
        "            \"Complete Tertiary Education\": 6,\n",
        "            \"Complete Secondary Education\": 5,\n",
        "            \"Incomplete Tertiary Education\": 4,\n",
        "            \"Complete Primary Education\": 3,\n",
        "            \"Incomplete Secondary Education\": 2,\n",
        "            \"Incomplete Primary Education\": 1,\n",
        "            \"Never attended\": 0\n",
        "        }\n",
        "        if not pd.api.types.is_numeric_dtype(df[\"educ_max\"]):\n",
        "            df[\"educ_max\"] = df[\"educ_max\"].map(educ_map).fillna(0).astype(int)\n",
        "        else:\n",
        "            df[\"educ_max\"] = df[\"educ_max\"].fillna(0).astype(int)\n",
        "\n",
        "    #one-hot Encoding\n",
        "    multi_cols = [c for c in [\"water_source\", \"sanitation_source\", \"sector1d\", \"dweltyp\"] if c in df.columns]\n",
        "    for col in multi_cols:\n",
        "        df[col] = df[col].fillna(\"Missing\")\n",
        "\n",
        "    df = pd.get_dummies(df, columns=multi_cols, prefix=multi_cols, drop_first=False)\n",
        "\n",
        "   # 3) 수치 결측: survey_id별 중앙값\n",
        "    num_cols = [\"utl_exp_ppp17\", \"share_secondary\"]\n",
        "    num_cols = [c for c in num_cols if c in df.columns]\n",
        "\n",
        "    if fit:\n",
        "        med = df.groupby(\"survey_id\")[num_cols].median()\n",
        "        state[\"num_medians_by_survey\"] = med.to_dict()\n",
        "\n",
        "    med_dict = state[\"num_medians_by_survey\"]\n",
        "    for c in num_cols:\n",
        "        df[c] = df[c].fillna(df[\"survey_id\"].map(med_dict[c]))\n",
        "\n",
        "\n",
        "    # 6) Asset PCA (PC1)\n",
        "    #    - train: asset 컬럼 선정 + scaler/pca fit 저장\n",
        "    #    - test : train에서 선정한 컬럼 + scaler/pca transform\n",
        "    base_asset_cols = [c for c in ['water', 'toilet', 'sewer', 'elect', 'urban', 'owner'] if c in df.columns]\n",
        "    asset_dummy_cols = [c for c in df.columns\n",
        "                        if c.startswith('dweltyp_') or c.startswith('water_source_') or c.startswith('sanitation_source_')]\n",
        "    asset_cols_all = base_asset_cols + asset_dummy_cols\n",
        "\n",
        "    if fit:\n",
        "        X_asset = df[asset_cols_all].fillna(0)\n",
        "        var = X_asset.var()\n",
        "        asset_cols_used = list(var[var > 0].index)\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        X_scaled = scaler.fit_transform(X_asset[asset_cols_used])\n",
        "\n",
        "        pca = PCA(n_components=1)\n",
        "        pc1 = pca.fit_transform(X_scaled).reshape(-1)\n",
        "\n",
        "        df[\"asset_wealth_index\"] = pc1\n",
        "\n",
        "        state[\"asset_cols_used\"] = asset_cols_used\n",
        "        state[\"asset_scaler\"] = scaler\n",
        "        state[\"asset_pca\"] = pca\n",
        "        state[\"asset_pca_var_ratio\"] = float(pca.explained_variance_ratio_[0])\n",
        "    else:\n",
        "        asset_cols_used = state.get(\"asset_cols_used\", [])\n",
        "        scaler = state.get(\"asset_scaler\", None)\n",
        "        pca = state.get(\"asset_pca\", None)\n",
        "\n",
        "        X_asset = df.reindex(columns=asset_cols_used, fill_value=0).fillna(0)\n",
        "        X_scaled = scaler.transform(X_asset)\n",
        "        pc1 = pca.transform(X_scaled).reshape(-1)\n",
        "\n",
        "        df[\"asset_wealth_index\"] = pc1\n",
        "\n",
        "    #EDR\n",
        "    adult_total = (df[\"num_adult_female\"].fillna(0) + df[\"num_adult_male\"].fillna(0))\n",
        "    workers = df[\"sworkershh\"].fillna(0) * adult_total\n",
        "\n",
        "    #demo\n",
        "    children_cols = [\"num_children5\", \"num_children10\", \"num_children18\"]\n",
        "    df[\"total_children\"] = df[children_cols].fillna(0).sum(axis=1)\n",
        "\n",
        "    adults_cols = [\"num_adult_female\", \"num_adult_male\"]\n",
        "    df[\"total_adults\"] = df[adults_cols].fillna(0).sum(axis=1)\n",
        "\n",
        "    # [NEW] 아동 부양비 (성인 1명당 감당해야 할 아이 수)\n",
        "    # 성인이 0명인 경우 대비하여 max(1) 처리\n",
        "    df[\"refined_dependency_ratio\"] = df[\"total_children\"] / df[\"total_adults\"].replace(0, 1)\n",
        "\n",
        "    # [NEW] 가구 내 아동 점유율 (hsize 대비)\n",
        "    df[\"child_share\"] = df[\"total_children\"] / df[\"hsize_fixed\"].replace(0, 1)\n",
        "\n",
        "    df[\"no_worker\"] = (workers == 0).astype(int)\n",
        "    df[\"edr_score\"] = np.where(workers > 0, (df[\"hsize_fixed\"].fillna(0) - workers) / workers, np.nan)\n",
        "\n",
        "\n",
        "    if 'sfworkershh' in df.columns:\n",
        "        df['sfworkershh'] = df['sfworkershh'].fillna(0)\n",
        "    else:\n",
        "        df['sfworkershh'] = 0\n",
        "\n",
        "    bins = [-np.inf, 0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
        "    labels = [0, 1, 2, 3, 4, 5]\n",
        "\n",
        "    df['sfworkershh_clip'] = df['sfworkershh'].clip(lower=0, upper=1)\n",
        "    # pd.cut을 사용하여 비율을 점수로 변환\n",
        "    df['sfworker_rank'] = pd.cut(df['sfworkershh_clip'], bins=bins, labels=labels, include_lowest=True)\n",
        "\n",
        "    # 카테고리 타입을 정수형(int)으로 변환 (모델 학습용)\n",
        "    df['sfworker_rank'] = df['sfworker_rank'].astype(int)\n",
        "\n",
        "    # 하나 더 추가 변수: 정규직 인원 수 (Quantity)\n",
        "    # 비율이 낮아도 가족 수가 많으면 중요하므로 이 정보는 살려둠\n",
        "    df['num_regular_workers'] = df['sfworkershh'] * df['hsize']\n",
        "\n",
        "    #여성가장\n",
        "    df['is_female_head'] = df['male'].apply(lambda x: 1 if x == 0 else 0)\n",
        "\n",
        "    #utl_exp_ppp17 log화\n",
        "    df[\"log_utl_exp_ppp17\"] = np.log(df[\"utl_exp_ppp17\"].fillna(0) + 1)\n",
        "\n",
        "    # Define q_low and q_high locally for this clipping logic\n",
        "    local_q_low, local_q_high = 0.01, 0.99 # Use consistent values, e.g., 1st and 99th percentile\n",
        "\n",
        "    if fit:\n",
        "      caps = df.groupby(\"survey_id\")[\"log_utl_exp_ppp17\"].quantile([local_q_low, local_q_high]).unstack()\n",
        "      # caps: index=survey_id, columns=[local_q_low, local_q_high]\n",
        "      state[\"utl_log_caps_by_survey\"] = caps.to_dict(orient=\"index\")\n",
        "      state[\"utl_log_cap_q_low\"] = local_q_low\n",
        "      state[\"utl_log_cap_q_high\"] = local_q_high\n",
        "      state[\"utl_log_cap_group\"] = \"survey_id\"\n",
        "\n",
        "\n",
        "    # 3) transform: 저장된 컷으로 clip\n",
        "    if state.get(\"utl_log_cap_group\") == \"survey_id\":\n",
        "        caps = state[\"utl_log_caps_by_survey\"]\n",
        "        # Retrieve q_low/q_high used during fitting from state\n",
        "        q_low_fitted = state[\"utl_log_cap_q_low\"]\n",
        "        q_high_fitted = state[\"utl_log_cap_q_high\"]\n",
        "\n",
        "        def clip_row(r):\n",
        "            sid = r[\"survey_id\"]\n",
        "            if sid in caps:\n",
        "                lo = caps[sid].get(q_low_fitted, None)\n",
        "                hi = caps[sid].get(q_high_fitted, None)\n",
        "                # q_low/q_high가 float key로 저장될 때 미세 오차가 걱정되면 아래 fallback 사용\n",
        "                if lo is None or hi is None:\n",
        "                    # 키가 문자열로 저장된 경우 대비\n",
        "                    lo = caps[sid].get(str(q_low_fitted), lo)\n",
        "                    hi = caps[sid].get(str(q_high_fitted), hi)\n",
        "                if lo is not None and hi is not None:\n",
        "                    return np.clip(r[\"log_utl_exp_ppp17\"], lo, hi) # Changed 'utl_log'\n",
        "            return r[\"log_utl_exp_ppp17\"] # Changed 'utl_log'\n",
        "\n",
        "        df[\"utl_log_qcap\"] = df.apply(clip_row, axis=1)\n",
        "\n",
        "    else:\n",
        "        caps_g = state[\"utl_log_caps_global\"]\n",
        "        df[\"utl_log_qcap\"] = df[\"log_utl_exp_ppp17\"].clip(lower=caps_g[\"lo\"], upper=caps_g[\"hi\"]) # Changed 'utl_log'\n",
        "\n",
        "    #weight\n",
        "    if fit:\n",
        "        wcap = df.groupby(\"survey_id\")[\"weight\"].quantile(0.999).to_dict()\n",
        "        state[\"weight_cap_999_by_survey\"] = wcap\n",
        "\n",
        "    wcap = state[\"weight_cap_999_by_survey\"]\n",
        "    df[\"weight_capped\"] = df.apply(lambda r: min(r[\"weight\"], wcap.get(r[\"survey_id\"], r[\"weight\"])), axis=1)\n",
        "\n",
        "    # Fill NaNs in object columns with a string placeholder for CatBoost compatibility\n",
        "    for col in df.select_dtypes(include=['object']).columns:\n",
        "        df[col] = df[col].astype(str).replace('nan', 'Missing_Category')\n",
        "\n",
        "    # 7) 최종 feature matrix 생성\n",
        "    #    - train: train_cols 저장\n",
        "    #    - test : train_cols에 reindex\n",
        "    drop_cols = []\n",
        "    for c in [\"cons_ppp17\", \"log_cons\", \"survey_id\", \"hhid\", \"sworkershh\", \"utl_exp_ppp17\", \"num_children5\", \"num_children10\", \"num_children18\",\"num_adult_female\", \"num_adult_male\" ]:\n",
        "        if c in df.columns:\n",
        "            drop_cols.append(c)\n",
        "\n",
        "    X = df.drop(columns=drop_cols)\n",
        "\n",
        "    if fit:\n",
        "        state[\"train_cols\"] = list(X.columns)\n",
        "    else:\n",
        "        X = X.reindex(columns=state[\"train_cols\"], fill_value=0)\n",
        "\n",
        "    return X, state"
      ],
      "metadata": {
        "id": "u5ITxMMqmueq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from scipy.stats import norm\n",
        "from sklearn.isotonic import IsotonicRegression\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import ElasticNetCV\n",
        "\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "from catboost import CatBoostRegressor\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 1) 기초 설정 및 Helper 함수 (원형 유지 + 필요한 최소 보강)\n",
        "# ============================================================\n",
        "\n",
        "DEFAULT_THRESHOLDS = np.array([\n",
        "    3.17, 3.94, 4.60, 5.26, 5.88, 6.47, 7.06, 7.70, 8.40,\n",
        "    9.13, 9.87, 10.70, 11.62, 12.69, 14.03, 15.64, 17.76, 20.99, 27.37\n",
        "], dtype=float)\n",
        "\n",
        "def infer_thresholds_from_train_rates(train_rates_gt: pd.DataFrame):\n",
        "    survey_col = None\n",
        "    for c in train_rates_gt.columns:\n",
        "        if c.lower() in [\"survey_id\", \"survey\", \"s_id\"]:\n",
        "            survey_col = c\n",
        "            break\n",
        "    if survey_col is None:\n",
        "        survey_col = train_rates_gt.columns[0]\n",
        "\n",
        "    rate_cols = [c for c in train_rates_gt.columns if c != survey_col]\n",
        "    thr_pairs = []\n",
        "    for c in rate_cols:\n",
        "        m = re.search(r\"(\\d+(\\.\\d+)?)\", str(c))\n",
        "        if m:\n",
        "            thr_pairs.append((float(m.group(1)), c))\n",
        "\n",
        "    if len(thr_pairs) >= 10:\n",
        "        thr_pairs.sort(key=lambda x: x[0])\n",
        "        thresholds = np.array([t for t, _ in thr_pairs], dtype=float)\n",
        "        ordered_cols = [col for _, col in thr_pairs]\n",
        "    else:\n",
        "        thresholds = DEFAULT_THRESHOLDS\n",
        "        ordered_cols = rate_cols\n",
        "\n",
        "    return thresholds, ordered_cols, survey_col\n",
        "\n",
        "def eval_rate_metrics(rate_pred_df, train_rates_gt, survey_col=\"survey_id\"):\n",
        "    rate_cols = [c for c in train_rates_gt.columns if c != survey_col]\n",
        "\n",
        "    gt = train_rates_gt.copy()\n",
        "    gt[survey_col] = gt[survey_col].astype(str)\n",
        "\n",
        "    rp = rate_pred_df.copy()\n",
        "    rp[survey_col] = rp[survey_col].astype(str)\n",
        "\n",
        "    merged = gt.merge(rp, on=survey_col, how=\"inner\", suffixes=('_gt', '_pred'))\n",
        "\n",
        "    gt_mat = merged[[c + \"_gt\" for c in rate_cols]].to_numpy(dtype=float)\n",
        "    pr_mat = merged[[c + \"_pred\" for c in rate_cols]].to_numpy(dtype=float)\n",
        "\n",
        "    diff = pr_mat - gt_mat\n",
        "    rmse = float(np.sqrt(np.mean(diff**2)))\n",
        "    r2 = float(r2_score(gt_mat.reshape(-1), pr_mat.reshape(-1)))\n",
        "    mse = float(np.mean(diff**2))\n",
        "\n",
        "    return rmse, mse, r2, rmse\n",
        "\n",
        "# ============================================================\n",
        "# (NEW) Survey-level sigma estimation (Method A)\n",
        "# ============================================================\n",
        "\n",
        "def make_survey_features_from_logmu(hh_df, mu_col, survey_col):\n",
        "    g = hh_df.groupby(survey_col)\n",
        "    out = pd.DataFrame({\n",
        "        survey_col: g.size().index,\n",
        "        \"n_households\": g.size().values,\n",
        "        \"mu_mean\": g[mu_col].mean().values,\n",
        "        \"mu_std\": g[mu_col].std().fillna(0).values,\n",
        "        \"mu_p90\": g[mu_col].quantile(0.9).values,\n",
        "        \"mu_p10\": g[mu_col].quantile(0.1).values,\n",
        "    })\n",
        "    return out\n",
        "\n",
        "\n",
        "def compute_survey_sigma_gt(\n",
        "    hh_df, mu_col, true_log_col, survey_col, weight_col\n",
        "):\n",
        "    tmp = hh_df[[survey_col, weight_col, mu_col, true_log_col]].copy()\n",
        "    tmp[\"resid2\"] = (tmp[true_log_col] - tmp[mu_col]) ** 2\n",
        "\n",
        "    sigma_gt = (\n",
        "        tmp.groupby(survey_col)\n",
        "        .apply(lambda g: np.sqrt(np.average(g.resid2, weights=g[weight_col])))\n",
        "        .reset_index(name=\"sigma_gt\")\n",
        "    )\n",
        "    return sigma_gt\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 2) CDF 기반 rate 계산 (중요: log-space 버전)\n",
        "#    이유: stacking이 log(cons)를 예측하므로 CDF도 log-space로 맞춰야 정합적임\n",
        "#    P(cons < z) = Phi((log(z) - mu_log)/sigma_log)\n",
        "# ============================================================\n",
        "\n",
        "def compute_survey_rates_CDF_log_sigma(\n",
        "    hh_df, mu_log_col, sigma_col,\n",
        "    survey_col, weight_col,\n",
        "    thresholds, monotone_fix=True\n",
        "):\n",
        "    tmp = hh_df[[survey_col, weight_col, mu_log_col, sigma_col]].copy()\n",
        "    tmp[survey_col] = tmp[survey_col].astype(str)\n",
        "\n",
        "    thresholds = np.array(thresholds, dtype=float)\n",
        "    logz = np.log(np.maximum(thresholds, 1e-12))\n",
        "    rate_cols = [f\"pct_hh_below_{t:.2f}\" for t in thresholds]\n",
        "\n",
        "    rows = []\n",
        "    for sid, g in tmp.groupby(survey_col):\n",
        "        w = g[weight_col].to_numpy(float)\n",
        "        mu = g[mu_log_col].to_numpy(float)\n",
        "        sigma = float(g[sigma_col].iloc[0])\n",
        "\n",
        "        z = (logz[None, :] - mu[:, None]) / (sigma + 1e-12)\n",
        "        probs = norm.cdf(z)\n",
        "\n",
        "        rates = np.average(probs, weights=w, axis=0)\n",
        "\n",
        "        row = {survey_col: int(sid) if sid.isdigit() else sid}\n",
        "        for i, c in enumerate(rate_cols):\n",
        "            row[c] = float(rates[i])\n",
        "\n",
        "        if monotone_fix:\n",
        "            arr = np.array([row[c] for c in rate_cols])\n",
        "            ir = IsotonicRegression(increasing=True, out_of_bounds=\"clip\")\n",
        "            fixed = ir.fit_transform(np.arange(len(arr)), arr)\n",
        "            for i, c in enumerate(rate_cols):\n",
        "                row[c] = float(np.clip(fixed[i], 0, 1))\n",
        "\n",
        "        rows.append(row)\n",
        "\n",
        "    return pd.DataFrame(rows)[[survey_col] + rate_cols]\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 3) Stacking 모델 (OOF + Test 예측 생성)  <= 네가 요청한 \"#이부분\" 대체\n",
        "#    - prepare_features(df, state, fit)은 네가 이미 만든 전처리 함수 사용\n",
        "#    - GroupKFold by survey_id\n",
        "#    - base: LGB/XGB/CAT, meta: ElasticNet\n",
        "#    - fold별로 \"train에서만\" 전처리 fit -> val/test transform (누수 방지)\n",
        "#    - meta도 fold별 train에서만 fit -> val/test 적용 (진짜 OOF)\n",
        "# ============================================================\n",
        "\n",
        "def clean_cols_unique(cols):\n",
        "    cleaned = [re.sub(r\"[^A-Za-z0-9_]+\", \"_\", str(c)) for c in cols]\n",
        "    seen = {}\n",
        "    out = []\n",
        "    for c in cleaned:\n",
        "        if c not in seen:\n",
        "            seen[c] = 0\n",
        "            out.append(c)\n",
        "        else:\n",
        "            seen[c] += 1\n",
        "            out.append(f\"{c}__dup{seen[c]}\")\n",
        "    return out\n",
        "\n",
        "def get_fold_logmu_preds_stacking(\n",
        "    train_full_df: pd.DataFrame,\n",
        "    test_full_df: pd.DataFrame,\n",
        "    n_splits: int = 3,\n",
        "    seed: int = 42,\n",
        "    q_low: float = 0.01,\n",
        "    q_high: float = 0.99,\n",
        "    lgb_params: dict = None,\n",
        "    xgb_params: dict = None,\n",
        "    cat_params: dict = None,\n",
        "    lgb_early_stopping: int = 300,\n",
        "    xgb_early_stopping: int = 300,\n",
        "    cat_od_wait: int = 300,\n",
        "    meta_cv: int = 5,\n",
        "    positive_meta: bool = True\n",
        "):\n",
        "    tr = train_full_df.copy()\n",
        "    te = test_full_df.copy()\n",
        "\n",
        "    if \"cons_ppp17\" not in tr.columns:\n",
        "        raise ValueError(\"train_full_df에 cons_ppp17이 필요합니다.\")\n",
        "    if \"survey_id\" not in tr.columns or \"survey_id\" not in te.columns:\n",
        "        raise ValueError(\"train/test에 survey_id가 필요합니다.\")\n",
        "    if \"weight\" not in tr.columns or \"weight\" not in te.columns:\n",
        "        raise ValueError(\"train/test에 weight가 필요합니다.\")\n",
        "\n",
        "    # log target\n",
        "    tr = tr[tr[\"cons_ppp17\"].notna() & (tr[\"cons_ppp17\"] > 0)].copy()\n",
        "    tr[\"log_cons\"] = np.log(tr[\"cons_ppp17\"].astype(float))\n",
        "\n",
        "    groups = tr[\"survey_id\"].astype(str).values\n",
        "    y_raw_log = tr[\"log_cons\"].values\n",
        "\n",
        "    # params default\n",
        "    if lgb_params is None:\n",
        "        lgb_params = dict(\n",
        "            n_estimators=8000,\n",
        "            learning_rate=0.011037005084629353,\n",
        "            num_leaves=32,\n",
        "            min_child_samples=80,\n",
        "            subsample=0.9943172425614316,\n",
        "            colsample_bytree=0.6340596394027656,\n",
        "            reg_lambda=0.017229667246557494,\n",
        "            random_state=seed,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "    if xgb_params is None:\n",
        "        xgb_params = dict(\n",
        "            n_estimators=4500,\n",
        "            learning_rate=0.01374925873088615,\n",
        "            max_depth=5,\n",
        "            subsample=0.7299796125452439,\n",
        "            colsample_bytree=0.6312082363473505,\n",
        "            reg_lambda=0.002584212748040726,\n",
        "            min_child_weight=9.320918627855733,\n",
        "            gamma=2.5538369849854422e-05,\n",
        "            objective=\"reg:squarederror\",\n",
        "            random_state=seed,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "    if cat_params is None:\n",
        "        cat_params = dict(\n",
        "            iterations=7500,\n",
        "            learning_rate=0.013781131984802358,\n",
        "            depth=9,\n",
        "            l2_leaf_reg=0.007607411541415501,\n",
        "            random_seed=seed,\n",
        "            loss_function=\"RMSE\",\n",
        "            verbose=False\n",
        "        )\n",
        "\n",
        "    n_tr = len(tr)\n",
        "    oof_logmu = np.zeros(n_tr, dtype=float)\n",
        "    y_oof_clip = np.zeros(n_tr, dtype=float)\n",
        "\n",
        "    # test 예측은 fold별 meta 예측을 평균\n",
        "    test_logmu_folds = []\n",
        "\n",
        "    gkf = GroupKFold(n_splits=n_splits)\n",
        "\n",
        "    for fold, (tr_idx, va_idx) in enumerate(gkf.split(tr, y_raw_log, groups=groups), 1):\n",
        "        tr_fold = tr.iloc[tr_idx].copy()\n",
        "        va_fold = tr.iloc[va_idx].copy()\n",
        "\n",
        "        # fold별 y clip (train 기준)\n",
        "        lo = float(np.quantile(tr_fold[\"log_cons\"].values, q_low))\n",
        "        hi = float(np.quantile(tr_fold[\"log_cons\"].values, q_high))\n",
        "        y_tr = tr_fold[\"log_cons\"].clip(lo, hi).values\n",
        "        y_va = va_fold[\"log_cons\"].clip(lo, hi).values\n",
        "        y_oof_clip[va_idx] = y_va\n",
        "\n",
        "        # fold별 전처리 fit/transform\n",
        "        X_tr, st = prepare_features(tr_fold, state=None, fit=True)\n",
        "        X_va, _  = prepare_features(va_fold, state=st, fit=False)\n",
        "        X_te, _  = prepare_features(te, state=st, fit=False)\n",
        "\n",
        "        # 컬럼명 정리 + train 기준 정렬\n",
        "        orig_cols = list(X_tr.columns)\n",
        "        new_cols = clean_cols_unique(orig_cols)\n",
        "        rename_map = dict(zip(orig_cols, new_cols))\n",
        "\n",
        "        X_tr = X_tr.rename(columns=rename_map)\n",
        "        X_va = X_va.rename(columns=rename_map).reindex(columns=X_tr.columns, fill_value=0)\n",
        "        X_te = X_te.rename(columns=rename_map).reindex(columns=X_tr.columns, fill_value=0)\n",
        "\n",
        "        # sample_weight (가능하면 사용, sqrt로 완화)\n",
        "        w_tr = tr_fold[\"weight\"].astype(float).values\n",
        "        w_va = va_fold[\"weight\"].astype(float).values\n",
        "        w_tr_use = np.sqrt(np.maximum(w_tr, 0))\n",
        "        w_va_use = np.sqrt(np.maximum(w_va, 0))\n",
        "\n",
        "        # base models\n",
        "        m_lgb = lgb.LGBMRegressor(**lgb_params)\n",
        "        try:\n",
        "            m_lgb.fit(\n",
        "                X_tr, y_tr,\n",
        "                sample_weight=w_tr_use,\n",
        "                eval_set=[(X_va, y_va)],\n",
        "                eval_sample_weight=[w_va_use],\n",
        "                eval_metric=\"rmse\",\n",
        "                callbacks=[lgb.early_stopping(lgb_early_stopping, verbose=False)]\n",
        "            )\n",
        "        except TypeError:\n",
        "            m_lgb.fit(\n",
        "                X_tr, y_tr,\n",
        "                sample_weight=w_tr_use,\n",
        "                eval_set=[(X_va, y_va)],\n",
        "                eval_metric=\"rmse\",\n",
        "                callbacks=[lgb.early_stopping(lgb_early_stopping, verbose=False)]\n",
        "            )\n",
        "        pred_lgb_va = m_lgb.predict(X_va, num_iteration=getattr(m_lgb, \"best_iteration_\", None))\n",
        "        pred_lgb_te = m_lgb.predict(X_te, num_iteration=getattr(m_lgb, \"best_iteration_\", None))\n",
        "\n",
        "        m_xgb = xgb.XGBRegressor(**xgb_params)\n",
        "        try:\n",
        "            m_xgb.fit(\n",
        "                X_tr, y_tr,\n",
        "                sample_weight=w_tr_use,\n",
        "                eval_set=[(X_va, y_va)],\n",
        "                sample_weight_eval_set=[w_va_use],\n",
        "                # eval_metric=\"rmse\",  # Removed this line\n",
        "                # early_stopping_rounds=xgb_early_stopping, # Removed this line\n",
        "                verbose=False\n",
        "            )\n",
        "        except TypeError:\n",
        "            m_xgb.fit(\n",
        "                X_tr, y_tr,\n",
        "                sample_weight=w_tr_use,\n",
        "                eval_set=[(X_va, y_va)],\n",
        "                # eval_metric=\"rmse\",  # Removed this line\n",
        "                # early_stopping_rounds=xgb_early_stopping, # Removed this line\n",
        "                verbose=False\n",
        "            )\n",
        "        best_it = getattr(m_xgb, \"best_iteration\", None)\n",
        "        if best_it is None:\n",
        "            pred_xgb_va = m_xgb.predict(X_va)\n",
        "            pred_xgb_te = m_xgb.predict(X_te)\n",
        "        else:\n",
        "            pred_xgb_va = m_xgb.predict(X_va, iteration_range=(0, int(best_it) + 1))\n",
        "            pred_xgb_te = m_xgb.predict(X_te, iteration_range=(0, int(best_it) + 1))\n",
        "\n",
        "        cat_params_fold = dict(cat_params)\n",
        "        cat_params_fold.update(dict(use_best_model=True, od_type=\"Iter\", od_wait=int(cat_od_wait)))\n",
        "        m_cat = CatBoostRegressor(**cat_params_fold)\n",
        "        m_cat.fit(\n",
        "            X_tr, y_tr,\n",
        "            sample_weight=w_tr_use,\n",
        "            eval_set=(X_va, y_va),\n",
        "            verbose=False\n",
        "        )\n",
        "        pred_cat_va = m_cat.predict(X_va)\n",
        "        pred_cat_te = m_cat.predict(X_te)\n",
        "\n",
        "        # meta train/val/test (fold train으로만 fit)\n",
        "        P_tr = np.vstack([\n",
        "            m_lgb.predict(X_tr, num_iteration=getattr(m_lgb, \"best_iteration_\", None)),\n",
        "            (m_xgb.predict(X_tr, iteration_range=(0, int(best_it) + 1)) if best_it is not None else m_xgb.predict(X_tr)),\n",
        "            m_cat.predict(X_tr)\n",
        "        ]).T\n",
        "\n",
        "        P_va = np.vstack([pred_lgb_va, pred_xgb_va, pred_cat_va]).T\n",
        "        P_te = np.vstack([pred_lgb_te, pred_xgb_te, pred_cat_te]).T\n",
        "\n",
        "        meta = Pipeline(steps=[\n",
        "            (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
        "            (\"enet\", ElasticNetCV(\n",
        "                l1_ratio=[0.05, 0.1, 0.2, 0.4, 0.6, 0.8, 0.95],\n",
        "                alphas=np.logspace(-4, 1, 40),\n",
        "                cv=int(meta_cv),\n",
        "                fit_intercept=True,\n",
        "                positive=bool(positive_meta),\n",
        "                random_state=seed,\n",
        "                max_iter=20000\n",
        "            ))\n",
        "        ])\n",
        "        meta.fit(P_tr, y_tr)\n",
        "\n",
        "        pred_meta_va = meta.predict(P_va)\n",
        "        pred_meta_te = meta.predict(P_te)\n",
        "\n",
        "        oof_logmu[va_idx] = pred_meta_va\n",
        "        test_logmu_folds.append(pred_meta_te)\n",
        "\n",
        "        rmse_fold = float(np.sqrt(mean_squared_error(y_va, pred_meta_va)))\n",
        "        print(f\"[Fold {fold}] meta log RMSE = {rmse_fold:.5f}\")\n",
        "\n",
        "    # test는 fold 평균\n",
        "    test_logmu = np.mean(np.vstack(test_logmu_folds), axis=0)\n",
        "\n",
        "    # base_rmse_log (alpha 튜닝 기준)\n",
        "    base_rmse_log = float(np.sqrt(np.mean((y_oof_clip - oof_logmu) ** 2)))\n",
        "\n",
        "    return oof_logmu, test_logmu, y_oof_clip, base_rmse_log\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 4) 통합 실행: Stacking OOF/Test -> Alpha 튜닝(CDF) -> 최종 rate 예측\n",
        "# ============================================================\n",
        "\n",
        "def pipeline_sigma_model_log_cdf_with_stacking(\n",
        "    train_full_df,\n",
        "    test_full_df,\n",
        "    train_rates_gt,\n",
        "    thresholds,\n",
        "    survey_col=\"survey_id\",\n",
        "    weight_col=\"weight\",\n",
        "    n_splits=3,\n",
        "    seed=42,\n",
        "    lgb_params=None,\n",
        "    xgb_params=None,\n",
        "    cat_params=None\n",
        "):\n",
        "    # 1) stacking\n",
        "    oof_logmu, test_logmu, y_oof_clip, _ = get_fold_logmu_preds_stacking(\n",
        "        train_full_df=train_full_df,\n",
        "        test_full_df=test_full_df,\n",
        "        n_splits=n_splits,\n",
        "        seed=seed,\n",
        "        lgb_params=lgb_params,\n",
        "        xgb_params=xgb_params,\n",
        "        cat_params=cat_params\n",
        "    )\n",
        "\n",
        "    # 2) sigma GT (train only)\n",
        "    train_eval = train_full_df.loc[\n",
        "        train_full_df[\"cons_ppp17\"].notna() & (train_full_df[\"cons_ppp17\"] > 0),\n",
        "        [survey_col, weight_col]\n",
        "    ].copy()\n",
        "    train_eval[\"oof_logmu\"] = oof_logmu\n",
        "    train_eval[\"log_cons\"] = np.log(train_full_df.loc[train_eval.index, \"cons_ppp17\"])\n",
        "\n",
        "    sigma_gt = compute_survey_sigma_gt(\n",
        "        train_eval,\n",
        "        mu_col=\"oof_logmu\",\n",
        "        true_log_col=\"log_cons\",\n",
        "        survey_col=survey_col,\n",
        "        weight_col=weight_col\n",
        "    )\n",
        "\n",
        "    # 3) survey-level features\n",
        "    feat_train = make_survey_features_from_logmu(\n",
        "        train_eval, \"oof_logmu\", survey_col\n",
        "    )\n",
        "    feat_train = feat_train.merge(sigma_gt, on=survey_col, how=\"left\")\n",
        "\n",
        "    X_sig = feat_train.drop(columns=[survey_col, \"sigma_gt\"])\n",
        "    y_sig = feat_train[\"sigma_gt\"]\n",
        "\n",
        "    sigma_model = CatBoostRegressor(\n",
        "        iterations=2000,\n",
        "        depth=6,\n",
        "        learning_rate=0.03,\n",
        "        loss_function=\"RMSE\",\n",
        "        random_seed=seed,\n",
        "        verbose=False\n",
        "    )\n",
        "    sigma_model.fit(X_sig, y_sig)\n",
        "\n",
        "    # 4) test sigma prediction\n",
        "    test_eval = test_full_df[[survey_col]].copy()\n",
        "    test_eval[\"pred_logmu\"] = test_logmu\n",
        "\n",
        "    feat_test = make_survey_features_from_logmu(\n",
        "        test_eval, \"pred_logmu\", survey_col\n",
        "    )\n",
        "    feat_test[\"sigma_hat\"] = sigma_model.predict(\n",
        "        feat_test.drop(columns=[survey_col])\n",
        "    )\n",
        "\n",
        "    sigma_map = dict(zip(feat_test[survey_col], feat_test[\"sigma_hat\"]))\n",
        "    test_eval[\"sigma_hat\"] = test_eval[survey_col].map(sigma_map)\n",
        "\n",
        "    # 5) final rate\n",
        "    test_eval[weight_col] = test_full_df[weight_col].values\n",
        "\n",
        "    final_rates = compute_survey_rates_CDF_log_sigma(\n",
        "        hh_df=test_eval,\n",
        "        mu_log_col=\"pred_logmu\",\n",
        "        sigma_col=\"sigma_hat\",\n",
        "        survey_col=survey_col,\n",
        "        weight_col=weight_col,\n",
        "        thresholds=thresholds,\n",
        "        monotone_fix=True\n",
        "    )\n",
        "\n",
        "    return final_rates, test_logmu\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 5) 실행 예시 (너가 이미 갖고 있는 변수 기준)\n",
        "# ============================================================\n",
        "\n",
        "# thresholds 추출\n",
        "thresholds, ordered_rate_cols, rates_survey_col = infer_thresholds_from_train_rates(train_rates_gt)\n",
        "print(\"Thresholds detected:\", len(thresholds))\n",
        "\n",
        "# train_rates_gt의 survey_col 정리 (필요 시)\n",
        "train_rates_gt_fixed = train_rates_gt.rename(columns={rates_survey_col: \"survey_id\"}).copy()\n",
        "\n",
        "# Define train_full and test_full\n",
        "train_full = df\n",
        "test_full = test_hh_features\n",
        "\n",
        "# optional: base model params (원하면 너 튜닝값 넣어도 됨)\n",
        "lgb_params = None\n",
        "xgb_params = None\n",
        "cat_params = None\n",
        "\n",
        "# Modified the function call to capture test_logmu\n",
        "final_rates_df_result, test_logmu_result = pipeline_sigma_model_log_cdf_with_stacking(\n",
        "    train_full_df=df,\n",
        "    test_full_df=test_hh_features,\n",
        "    train_rates_gt=train_rates_gt_fixed,\n",
        "    thresholds=thresholds,\n",
        "    survey_col=\"survey_id\",\n",
        "    weight_col=\"weight\"\n",
        ")\n",
        "\n",
        "rename_map = {\n",
        "    f\"pct_hh_below_{thr:.2f}\": col\n",
        "    for thr, col in zip(thresholds, ordered_rate_cols)\n",
        "}\n",
        "print(final_rates_df_result.head())\n",
        "submission = final_rates_df_result.rename(columns=rename_map)\n",
        "submission.to_csv(\"submission_sigma_model.csv\", index=False)\n",
        "print(\"Saved submission_sigma_model.csv\")\n",
        "\n",
        "# Assign to global variables that the next cell expects\n",
        "final_rates_df = final_rates_df_result\n",
        "test_logmu = test_logmu_result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtGEf4lAmv4N",
        "outputId": "3388a47b-fdbd-44b5-c2f8-5a18ae98ef05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thresholds detected: 19\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.047004 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2002\n",
            "[LightGBM] [Info] Number of data points in the train set: 66772, number of used features: 128\n",
            "[LightGBM] [Info] Start training from score 2.160551\n",
            "[Fold 1] meta log RMSE = 0.33378\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058068 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1987\n",
            "[LightGBM] [Info] Number of data points in the train set: 69650, number of used features: 129\n",
            "[LightGBM] [Info] Start training from score 2.169405\n",
            "[Fold 2] meta log RMSE = 0.33483\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.086141 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1984\n",
            "[LightGBM] [Info] Number of data points in the train set: 72046, number of used features: 129\n",
            "[LightGBM] [Info] Start training from score 2.181899\n",
            "[Fold 3] meta log RMSE = 0.33655\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1703902397.py:98: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(lambda g: np.sqrt(np.average(g.resid2, weights=g[weight_col])))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   survey_id  pct_hh_below_3.17  pct_hh_below_3.94  pct_hh_below_4.60  \\\n",
            "0     400000           0.050216           0.097544           0.145102   \n",
            "1     500000           0.043728           0.089654           0.137254   \n",
            "2     600000           0.044964           0.089282           0.134852   \n",
            "\n",
            "   pct_hh_below_5.26  pct_hh_below_5.88  pct_hh_below_6.47  pct_hh_below_7.06  \\\n",
            "0           0.196088           0.245289           0.292141           0.338229   \n",
            "1           0.189095           0.239544           0.287778           0.335299   \n",
            "2           0.184701           0.233631           0.280838           0.327732   \n",
            "\n",
            "   pct_hh_below_7.70  pct_hh_below_8.40  pct_hh_below_9.13  pct_hh_below_9.87  \\\n",
            "0           0.386684           0.437241           0.486776           0.533388   \n",
            "1           0.385262           0.437337           0.488259           0.536054   \n",
            "2           0.377400           0.429502           0.480721           0.528985   \n",
            "\n",
            "   pct_hh_below_10.70  pct_hh_below_11.62  pct_hh_below_12.69  \\\n",
            "0            0.581241            0.628907            0.677633   \n",
            "1            0.584970            0.633517            0.682932   \n",
            "2            0.578523            0.627784            0.677979   \n",
            "\n",
            "   pct_hh_below_14.03  pct_hh_below_15.64  pct_hh_below_17.76  \\\n",
            "0            0.729471            0.780194            0.831605   \n",
            "1            0.735238            0.786124            0.837382   \n",
            "2            0.731114            0.782754            0.834665   \n",
            "\n",
            "   pct_hh_below_20.99  pct_hh_below_27.37  \n",
            "0            0.885784            0.943593  \n",
            "1            0.891036            0.947742  \n",
            "2            0.888851            0.946069  \n",
            "Saved submission_sigma_model.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Submission 만들기 (submission.zip)\n",
        "# - root level에 2개 CSV 포함:\n",
        "#   1) predicted_household_consumption.csv\n",
        "#   2) predicted_poverty_distribution.csv\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# -------------------------------\n",
        "# [전제]\n",
        "# 아래 변수들이 이미 만들어져 있어야 함:\n",
        "# 1) test_full : test household 단위 DataFrame (survey_id, hhid(or household_id) 포함)\n",
        "# 2) (stacking+CDF) 파이프라인 결과:\n",
        "#    - test_logmu : test household별 log(consumption) 예측 (길이 = len(test_full))\n",
        "#    - final_rates_df : survey별 poverty distribution DataFrame\n",
        "#      (컬럼: survey_id, pct_hh_below_3.17 ... pct_hh_below_27.37)\n",
        "#\n",
        "# 만약 너가 위에서 만든 함수 리턴을 그대로 받았다면,\n",
        "# final_rates_df 는 `final_rates_df`로 있고,\n",
        "# test_logmu는 pipeline에서 따로 리턴하지 않으니 아래에서 다시 만들거나,\n",
        "# pipeline 함수에서 test_logmu를 추가로 리턴하도록 수정해야 함.\n",
        "# -------------------------------\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 1) predicted_household_consumption.csv 만들기\n",
        "#    - 요구: survey_id, household_id, per_capita_household_consumption\n",
        "# ============================================================\n",
        "\n",
        "# (A) test_logmu 준비\n",
        "# 만약 네 코드에서 test_logmu를 이미 갖고 있으면 그대로 사용.\n",
        "# 없으면 아래처럼 pipeline에서 나온 test_logmu를 변수로 확보해야 함.\n",
        "\n",
        "# 여기서는 test_logmu가 이미 존재한다고 가정:\n",
        "# test_logmu: shape (len(test_full), )\n",
        "\n",
        "if \"test_logmu\" not in globals():\n",
        "    raise ValueError(\n",
        "        \"test_logmu 변수가 없습니다. \"\n",
        "        \"stacking 예측 함수(get_fold_logmu_preds_stacking)에서 test_logmu를 받아서 저장해두세요.\"\n",
        "    )\n",
        "\n",
        "# (B) log -> raw consumption (per capita)\n",
        "test_pred_cons = np.exp(np.asarray(test_logmu, dtype=float))\n",
        "\n",
        "# (C) household consumption submission DF\n",
        "hh_sub = pd.DataFrame({\n",
        "    \"survey_id\": test_full[\"survey_id\"].astype(int),\n",
        "    \"hhid\": test_full[\"hhid\"].astype(int),\n",
        "    \"cons_ppp17\": test_pred_cons.astype(float)\n",
        "})\n",
        "\n",
        "# (D) 정렬(권장: survey_id, household_id)\n",
        "hh_sub = hh_sub.sort_values([\"survey_id\", \"hhid\"]).reset_index(drop=True)\n",
        "\n",
        "# (E) 행 수/결측 체크(필수 안정장치)\n",
        "if hh_sub.isna().any().any():\n",
        "    bad = hh_sub.isna().sum()\n",
        "    raise ValueError(f\"household submission에 NaN이 있습니다:\\n{bad}\")\n",
        "\n",
        "if len(hh_sub) != len(test_full):\n",
        "    raise ValueError(f\"행 수 불일치: household submission({len(hh_sub)}) != test_full({len(test_full)})\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 2) predicted_poverty_distribution.csv 만들기\n",
        "#    - 요구: survey_id + 19개 threshold 컬럼\n",
        "#    - final_rates_df는 이미 (survey_id, pct_hh_below_3.17 ... ) 형태여야 함\n",
        "# ============================================================\n",
        "\n",
        "if \"final_rates_df\" not in globals():\n",
        "    raise ValueError(\n",
        "        \"final_rates_df 변수가 없습니다. \"\n",
        "        \"stacking+CDF 파이프라인 결과(final_rates_df)를 받아서 저장해두세요.\"\n",
        "    )\n",
        "\n",
        "rate_sub = final_rates_df.copy()\n",
        "\n",
        "# (A) survey_id 타입/정렬\n",
        "rate_sub[\"survey_id\"] = rate_sub[\"survey_id\"].astype(int)\n",
        "rate_sub = rate_sub.sort_values(\"survey_id\").reset_index(drop=True)\n",
        "\n",
        "# (B) 필요한 컬럼 개수 체크: 1(survey_id)+19(threshold) = 20\n",
        "expected_rate_cols = [\"survey_id\"] + [c for c in rate_sub.columns if c.startswith(\"pct_hh_below_\")]\n",
        "if len(expected_rate_cols) != 20:\n",
        "    raise ValueError(\n",
        "        f\"poverty distribution 컬럼 개수가 요구사항과 다릅니다. \"\n",
        "        f\"(현재 {len(expected_rate_cols)}개; 요구 20개)\\n\"\n",
        "        f\"현재 threshold 컬럼들: {[c for c in rate_sub.columns if c.startswith('pct_hh_below_')]}\"\n",
        "    )\n",
        "\n",
        "# (C) threshold 컬럼들을 숫자 기준으로 정렬(3.17 -> 27.37 순서)\n",
        "thr_cols = [c for c in rate_sub.columns if c.startswith(\"pct_hh_below_\")]\n",
        "thr_pairs = []\n",
        "for c in thr_cols:\n",
        "    # c = \"pct_hh_below_3.17\"\n",
        "    m = re.search(r\"(\\d+(\\.\\d+)?)\", c)\n",
        "    if m:\n",
        "        thr_pairs.append((float(m.group(1)), c))\n",
        "thr_pairs.sort(key=lambda x: x[0])\n",
        "ordered_thr_cols = [c for _, c in thr_pairs]\n",
        "\n",
        "rate_sub = rate_sub[[\"survey_id\"] + ordered_thr_cols].copy()\n",
        "\n",
        "# (D) 값 범위 체크: [0,1]\n",
        "for c in ordered_thr_cols:\n",
        "    if ((rate_sub[c] < -1e-9) | (rate_sub[c] > 1 + 1e-9)).any():\n",
        "        raise ValueError(f\"rate 컬럼 {c}에 [0,1] 범위를 벗어난 값이 있습니다.\")\n",
        "\n",
        "# (E) survey 개수 체크(문서상 3개여야 함)\n",
        "# 대회 데이터가 3개 survey가 맞다는 전제일 때만 강제\n",
        "# 아니라면 아래 if 블록은 주석 처리\n",
        "if len(rate_sub) != 3:\n",
        "    print(f\"[경고] poverty distribution 행 수가 3이 아닙니다: {len(rate_sub)}행 (데이터 스펙 확인 필요)\")\n",
        "\n",
        "# ============================================================\n",
        "# 3) 파일 저장 + ZIP 생성 (root level 2 files)\n",
        "# ============================================================\n",
        "\n",
        "out_dir = \"submission_build\"\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "hh_path = os.path.join(out_dir, \"predicted_household_consumption.csv\")\n",
        "rate_path = os.path.join(out_dir, \"predicted_poverty_distribution.csv\")\n",
        "zip_path = os.path.join(out_dir, \"submission.zip\")\n",
        "\n",
        "hh_sub.to_csv(hh_path, index=False)\n",
        "rate_sub.to_csv(rate_path, index=False)\n",
        "\n",
        "# ZIP: root level에 두 파일만 들어가도록 arcname 지정\n",
        "with zipfile.ZipFile(zip_path, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
        "    zf.write(hh_path, arcname=\"predicted_household_consumption.csv\")\n",
        "    zf.write(rate_path, arcname=\"predicted_poverty_distribution.csv\")\n",
        "\n",
        "print(\"✅ Saved:\")\n",
        "print(\" -\", hh_path)\n",
        "print(\" -\", rate_path)\n",
        "print(\"✅ Zip created:\")\n",
        "print(\" -\", zip_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQrmtkgrm1nn",
        "outputId": "9c1d78ca-2654-4a24-c606-47a4f1315f2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved:\n",
            " - submission_build/predicted_household_consumption.csv\n",
            " - submission_build/predicted_poverty_distribution.csv\n",
            "✅ Zip created:\n",
            " - submission_build/submission.zip\n"
          ]
        }
      ]
    }
  ]
}